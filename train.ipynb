{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPFRqKyS75Sx7xAptYhlRxQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitgupta1998/Doodle_recogn_BEproject/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0ECv9kjDwO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import test as test #file name, look how to import colab file\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4FWd6AX_rEt",
        "colab_type": "code",
        "outputId": "818a4e2b-1732-4baf-fe61-2b33b74e1029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CId4TMBERyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir_data='/content/drive/My Drive/BE_project_dataset_and_code/data/'    \n",
        "def create_dic(dir_data):\n",
        "    dict={}\n",
        "    i=0\n",
        "    for file in sorted(os.listdir('/content/drive/My Drive/BE_project_dataset_and_code/data/')):\n",
        "        if file.endswith(\".npy\"):\n",
        "            str=file.split(\".\")\n",
        "            dict[i]=str[0]\n",
        "            i=i+1\n",
        "    return i,dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evd5k9cOEShs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class cnn:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 128\n",
        "        self.dir_data='/content/drive/My Drive/BE_project_dataset_and_code/data/'\n",
        "        self.num_of_classes,self.dict =create_dic(self.dir_data)\n",
        "        self.image_size = 28\n",
        "        self.validate_data = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM62gAtbElCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SVHN_net_v0(x_,num_of_classes):\n",
        "    with tf.variable_scope(\"CNN\"):\n",
        "        conv1 = tf.layers.conv2d(\n",
        "                                 inputs=x_,\n",
        "                                 filters=32,  # number of filters\n",
        "                                 kernel_size=[5, 5],\n",
        "                                 padding=\"same\",\n",
        "                                 activation=tf.nn.relu)\n",
        "            \n",
        "        pool1 = tf.layers.max_pooling2d(inputs=conv1,pool_size=[2, 2], strides=2)  # convolution stride\n",
        "        conv2 = tf.layers.conv2d(\n",
        "                                  inputs=pool1,\n",
        "                                  filters=32, # number of filters\n",
        "                                  kernel_size=[5, 5],\n",
        "                                  padding=\"same\",\n",
        "                                  activation=tf.nn.relu)\n",
        "                                 \n",
        "        pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
        "                                        pool_size=[2, 2],\n",
        "                                        strides=2)  # convolution stride\n",
        "                                        \n",
        "        conv3 = tf.layers.conv2d(\n",
        "                                 inputs=pool2,\n",
        "                                 filters=32, # number of filters\n",
        "                                  kernel_size=[5, 5],\n",
        "                                  padding=\"same\",\n",
        "                                  activation=tf.nn.relu)\n",
        "                                        \n",
        "        pool3 = tf.layers.max_pooling2d(inputs=conv3,\n",
        "                                        pool_size=[2, 2],\n",
        "                                        strides=2)\n",
        "        \n",
        "        conv4 = tf.layers.conv2d(\n",
        "                                 inputs=pool3,\n",
        "                                 filters=32, # number of filters\n",
        "                                 kernel_size=[5, 5],\n",
        "                                 padding=\"same\",\n",
        "                                 activation=tf.nn.relu)\n",
        "            \n",
        "        pool4 = tf.layers.max_pooling2d(inputs=conv4,\n",
        "                                        pool_size=[2, 2],\n",
        "                                        strides=2)\n",
        "                                 \n",
        "        pool_flat = tf.contrib.layers.flatten(pool4, scope='pool2flat')\n",
        "        dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
        "        logits = tf.layers.dense(inputs=dense, units=num_of_classes)\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm_WCEJ2Et1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_classification_loss(cnn,model_function):\n",
        "    with tf.Graph().as_default() as g:\n",
        "        with tf.device(\"/cpu:0\"):  # use gpu:0 if on GPU\n",
        "            x_ = tf.placeholder(tf.float32, [None, cnn.image_size, cnn.image_size,1],name='x')\n",
        "            y_ = tf.placeholder(tf.int32, [None],name='y')\n",
        "            y_logits = model_function(x_,cnn.num_of_classes)\n",
        "            \n",
        "            y_dict = dict(labels=y_, logits=y_logits)\n",
        "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
        "            cross_entropy_loss = tf.reduce_mean(losses)\n",
        "            trainer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "            train_op = trainer.minimize(cross_entropy_loss)\n",
        "            y_pred = tf.argmax(tf.nn.softmax(y_logits), axis=1)\n",
        "            correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    model_dict = {'graph': g, 'inputs': [x_, y_], 'train_op': train_op,\n",
        "                               'accuracy': accuracy, 'loss': cross_entropy_loss}\n",
        "\n",
        "    return model_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "960J-rWHExvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(cnn,x,y,i):\n",
        "    start=i*cnn.batch_size\n",
        "    end=start+cnn.batch_size\n",
        "    if (end>x.shape[0]):\n",
        "        end=x.shape[0]\n",
        "    x_batch_data=x[start:end,:,:,:]\n",
        "    y_batch_data=y[start:end]\n",
        "    return x_batch_data,y_batch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBG5bSErE2Tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(cnn,model_dict, x_data,y_data,x_test,y_test ,epoch_n, print_every):\n",
        "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
        "        saver = tf.train.Saver()\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        batch_num=int(np.ceil(x_data.shape[0]/cnn.batch_size))\n",
        "        for epoch_i in range(epoch_n):\n",
        "            for iter_i in range(batch_num):\n",
        "                x_placeholder=model_dict['inputs'][0]\n",
        "                y_placeholder=model_dict['inputs'][1]\n",
        "                #train_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
        "                [x_batch_data,y_batch_data]=get_data(cnn,x_data,y_data,iter_i)\n",
        "                sess.run(model_dict['train_op'], feed_dict={x_placeholder:x_batch_data,y_placeholder:y_batch_data})\n",
        "\n",
        "                if (iter_i%200==0):\n",
        "                    to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
        "                    loss,accuracy=sess.run(to_compute, feed_dict={x_placeholder:x_test,y_placeholder:y_test})\n",
        "                    print(iter_i,\"/\",batch_num,\"loss:\",loss,\" accuracy:\",accuracy)\n",
        "        saver.save(sess, \"./saved_sess/model.ckpt\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWGTc35ME7Bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(cnn,args):\n",
        "    dir_data='/content/drive/My Drive/BE_project_dataset_and_code/data/'\n",
        "    num_of_classess,dict=create_dic(dir_data)\n",
        "    data_l=np.zeros((1))\n",
        "    data_d=np.zeros((1,cnn.image_size*cnn.image_size))\n",
        "    index=1\n",
        "    for file in sorted(os.listdir(dir_data)):\n",
        "        if file.endswith(\".npy\"):\n",
        "            print(data_l.shape,data_d.shape,\"cur label num!\",index,file)\n",
        "            curr_data=np.load(dir_data+file)\n",
        "            data_size=curr_data.shape\n",
        "            #take only 30 percent of the data\n",
        "            #part_data=int((int(args.draws_per_class)/100)*(data_size[0]))\n",
        "            part_data=int((int(args[1])/100)*(data_size[0]))\n",
        "            curr_data=curr_data[1:part_data,:]\n",
        "            \n",
        "            #change to white background\n",
        "            curr_data=255-curr_data;\n",
        "            data_d=np.concatenate((data_d,curr_data))\n",
        "            data_l=np.concatenate((data_l,np.ones(curr_data.shape[0])*index))\n",
        "            if (index==int(args[2])):\n",
        "                break\n",
        "            index=index+1\n",
        "    data_l=np.expand_dims(data_l,1)\n",
        "    data_all=np.concatenate((data_d,data_l),axis=1)\n",
        "    data_all=np.random.permutation(data_all)\n",
        "\n",
        "    x_data=data_all[:,0:-1]\n",
        "    y_data=data_all[:,-1]\n",
        "    num_img=x_data.shape[0]\n",
        "    data_img=np.reshape(x_data,[num_img,cnn.image_size,cnn.image_size])\n",
        "   \n",
        "    \n",
        "    data_train=data_img[cnn.validate_data:,:,:]\n",
        "    data_train=np.expand_dims(data_train,3)\n",
        "\n",
        "    labels_train=y_data[cnn.validate_data:]\n",
        "    data_test=data_img[:cnn.validate_data:,:,:]\n",
        "    data_test=np.expand_dims(data_test,3)\n",
        "\n",
        "    labels_test=y_data[:cnn.validate_data]\n",
        "\n",
        "    \n",
        "    return data_train,labels_train,data_test,labels_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rri2Acu_E_mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(args):\n",
        "    quick_draw_cnn=cnn()\n",
        "    [x_data,y_data,x_test,y_test]=load_data(quick_draw_cnn,args)\n",
        "\n",
        "    model_dict = apply_classification_loss(quick_draw_cnn,SVHN_net_v0)\n",
        "    #train_model(quick_draw_cnn,model_dict, x_data,y_data,x_test,y_test ,epoch_n=int(args.epoch), print_every=20)\n",
        "    train_model(quick_draw_cnn,model_dict, x_data,y_data,x_test,y_test ,epoch_n=int(args[0]), print_every=20)\n",
        "\n",
        "\n",
        "    #test test data after finishing training\n",
        "    y_predicted=test.test_cnn(quick_draw_cnn,x_test,y_test)\n",
        "\n",
        "    mistakes=np.nonzero(y_predicted-y_test)\n",
        "    #mistakes is tuple,take the array only\n",
        "    mistakes=mistakes[0]\n",
        "    error_rate=mistakes.shape[0]/y_test.shape[0]\n",
        "    print(\"-----------------------------------------------------------------------\")\n",
        "    print(\"Final accuracy is :\",1-error_rate)\n",
        "    print(\"-----------------------------------------------------------------------\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBogQ5TJFVpH",
        "colab_type": "code",
        "outputId": "d24d14bf-7be6-4db3-9eb7-8f1f4489fd4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-ep', '--epoch', default=10)\n",
        "    parser.add_argument('-dpc', '--draws_per_class', default=30)\n",
        "    parser.add_argument('-class_num', '--class_num', default=30)\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    \"\"\"\n",
        "    args=[10,30,30]\n",
        "\n",
        "    print(\"-----------------------------------------------------------------------\")\n",
        "    #print(\"Train CNN with \" ,args.epoch,\"epochs\",args.class_num,\"classes\",args.draws_per_class,\"percent draws from each class\")\n",
        "    print(\"Train CNN with \" ,args[0],\"epochs\",args[1],\"classes\",args[2],\"percent draws from each class\")\n",
        "    \n",
        "    print(\"-----------------------------------------------------------------------\")\n",
        "    main(args)\n",
        "    #main(x,y,z)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------\n",
            "Train CNN with  10 epochs 30 classes 30 percent draws from each class\n",
            "-----------------------------------------------------------------------\n",
            "(1,) (1, 784) cur label num! 1 ambulance.npy\n",
            "(44401,) (44401, 784) cur label num! 2 apple.npy\n",
            "(87816,) (87816, 784) cur label num! 3 axe.npy\n",
            "(125051,) (125051, 784) cur label num! 4 basketball.npy\n",
            "(165187,) (165187, 784) cur label num! 5 bicycle.npy\n",
            "(203144,) (203144, 784) cur label num! 6 boomerang.npy\n",
            "(245947,) (245947, 784) cur label num! 7 butterfly.npy\n",
            "(281345,) (281345, 784) cur label num! 8 car.npy\n",
            "(336173,) (336173, 784) cur label num! 9 carrot.npy\n",
            "(375909,) (375909, 784) cur label num! 10 cat.npy\n",
            "(412868,) (412868, 784) cur label num! 11 clock.npy\n",
            "(449027,) (449027, 784) cur label num! 12 cookie.npy\n",
            "(488431,) (488431, 784) cur label num! 13 cup.npy\n",
            "(527646,) (527646, 784) cur label num! 14 donut.npy\n",
            "(569870,) (569870, 784) cur label num! 15 envelope.npy\n",
            "(610327,) (610327, 784) cur label num! 16 flower.npy\n",
            "(653771,) (653771, 784) cur label num! 17 hammer.npy\n",
            "(689473,) (689473, 784) cur label num! 18 key.npy\n",
            "(737761,) (737761, 784) cur label num! 19 knife.npy\n",
            "(789556,) (789556, 784) cur label num! 20 lightning.npy\n",
            "(835023,) (835023, 784) cur label num! 21 pencil.npy\n",
            "(871622,) (871622, 784) cur label num! 22 pizza.npy\n",
            "(910732,) (910732, 784) cur label num! 23 rainbow.npy\n",
            "(948784,) (948784, 784) cur label num! 24 snake.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v6DWiBxFX57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcUvfw4HYZh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}